---
title: "ADS503_Project"
author: "Christian Lee, Askhat Patni, Gagandeep Singh"
date: "2025-06-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ADS-503 Final Project

### Import Libraries

```{r}
library(tidyverse)    
library(skimr)        
library(corrplot)     
library(GGally)       
library(patchwork)  
library(gridExtra)
library(DataExplorer)
library(randomForest)
library(caret)
library(dplyr)
library(tidyr)
library(pROC)
```

## Load Dataset

```{r}
sleep_data <- read.csv("data/Sleep_health_and_lifestyle_dataset.csv")
sleep_data <- sleep_data %>%
  separate(Blood.Pressure, into = c("Systolic", "Diastolic"), sep = "/", convert = TRUE)

occupation_levels <- unique(sleep_data$Occupation)
sleep_data$Occupation <- factor(sleep_data$Occupation, levels = occupation_levels)

bmi_levels <- unique(sleep_data$BMI.Category)
sleep_data$BMI.Category <- factor(sleep_data$BMI.Category, levels = bmi_levels)

head(sleep_data)
```

## Exploratory Data Analysis

```{r}
# Summarize Data Set
skim(sleep_data)
```

```{r}
#Numeric Feature Distributions
sleep_data %>%
  select(where(is.numeric)) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  geom_histogram(fill = "steelblue", color = "black", bins = 20) +
  theme_minimal()

```

```{r}
#Categorical Feature Distributions
sleep_data %>%
  select(where(is.factor), Gender, Occupation, BMI.Category, Sleep.Disorder) %>%
  gather() %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ key, scales = "free", ncol = 2) +
  geom_bar(fill = "steelblue", color = "black",) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#Correlation Matrix 
numeric_data <- sleep_data %>%
  select(where(is.numeric))

cor_matrix <- cor(numeric_data, use = "complete.obs")

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.cex = 0.8)
```

```{r}
#Create Binary outcome column 
sleep_data <- sleep_data %>%
  mutate(Has.Sleep.Disorder = ifelse(Sleep.Disorder == "None", 0, 1))

# Inspect class ratio
table(sleep_data$Has.Sleep.Disorder)
```

```{r}
#Show relationship between numeric predictors and Binary outcome
numeric_features <- sleep_data %>%
  select(where(is.numeric)) %>%
  select(-Person.ID, -Has.Sleep.Disorder) %>%
  names()

plot_list <- lapply(numeric_features, function(feature) {
  ggplot(sleep_data, aes(x = factor(Has.Sleep.Disorder), y = .data[[feature]], fill = factor(Has.Sleep.Disorder))) +
    geom_boxplot() +
    labs(title = feature, x = "Has Sleep Disorder", y = feature) +
    theme_minimal() +
    theme(legend.position = "none")
})

grid.arrange(grobs = plot_list, ncol = 3, nrow = ceiling(length(plot_list)/3))


```


## Pre-processing

```{r}
# impute with KNN, center, and scale the data
preprocess_model <- preProcess(X.train, method = c("knnImpute")

# apply preprocess to train and test sets
X.train.pp <- predict(preprocess_model, newdata = X.train)
X.test.pp <- predict(preprocess_model, newdata = X.test)
```


```{r}
# Data splitting
# Set seed for reproducibility

set.seed(123)

# Create binary target variable
sleep_data$Has.Sleep.Disorder <- ifelse(sleep_data$Sleep.Disorder == "None", 0, 1)

# Initial stratified split: 75% for training + validation, 25% for testing
initial_split <- createDataPartition(sleep_data$Has.Sleep.Disorder, p = 0.75, list = FALSE)
train_val_data <- sleep_data[initial_split, ]
test_data <- sleep_data[-initial_split, ]

# Second stratified split: 80% of train_val_data for training, 20% for validation
train_split <- createDataPartition(train_val_data$Has.Sleep.Disorder, p = 0.8, list = FALSE)
train_data <- train_val_data[train_split, ]
val_data <- train_val_data[-train_split, ]

# Check proportions
prop.table(table(train_data$Has.Sleep.Disorder))
prop.table(table(val_data$Has.Sleep.Disorder))
prop.table(table(test_data$Has.Sleep.Disorder))
```

## Model Development 

### Setup 

```{r}

X_train <- train_data %>% select(-1, -(ncol(.)-1), -(ncol(.)))

y_train <- train_data %>% pull(last_col()) %>% factor(levels = c(0, 1), labels = c("No", "Yes"))

X_val <- val_data %>% select(-1, -(ncol(.)-1), -(ncol(.)))
y_val <- val_data %>% pull(last_col()) %>% factor(levels = c(0, 1), labels = c("No", "Yes"))

X_test <- test_data %>% select(-1, -(ncol(.)-1), -(ncol(.)))
y_test <- test_data %>% pull(last_col()) %>% factor(levels = c(0, 1), labels = c("No", "Yes"))

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
                     
eval_df <- data.frame(
  Model = character(),
  Accuracy = numeric(),
  Error_Rate = numeric(),
  Specificity = numeric(),
  Sensitivity = numeric(),
  Precision = numeric(),
  Recall = numeric(),
  F1_Score = numeric(),
  AUC = numeric(),
  stringsAsFactors = FALSE
)
```

### Neural Network

```{r, message=FALSE, warning=FALSE}
#Train Model
nnetGrid <- expand.grid(decay = c(0, 0.01, .1, .5, 1), 
                        size = c(1, 3, 7, 11, 13))
max_size <- max(nnetGrid$size)

num_inputs <- ncol(X_train)  
num_hidden <- max_size       
num_outputs <- length(levels(y_train))  

max_weights_needed <- (num_inputs + 1) * num_hidden + (num_hidden + 1) * num_outputs

set.seed(123)
nnet_model <- train(x = X_train, y = y_train,
                    method = "nnet",
                    tuneGrid = nnetGrid,
                    trControl = ctrl,
                    metric = "ROC",
                    linout = FALSE,
                    trace = FALSE,
                    MaxNWts = max_weights_needed,
                    maxit = 1000)

#Predict on val data
nnet_preds <- predict(nnet_model, X_val)
nnet_obs <- y_val
nnet_preds <- factor(nnet_preds, levels = c("No", "Yes"))
nnet_obs   <- factor(nnet_obs,   levels = c("No", "Yes"))

nnet_probs <- predict(nnet_model, X_val, type = "prob")[, "Yes"]

#Calculate metrics for eval_df
nnet_cm <- confusionMatrix(nnet_preds, nnet_obs, positive = "Yes")

nnet_accuracy <- nnet_cm$overall["Accuracy"]
nnet_error_rate <- 1 - nnet_accuracy
nnet_sensitivity <- nnet_cm$byClass["Sensitivity"]
nnet_specificity <- nnet_cm$byClass["Specificity"]
nnet_precision <- nnet_cm$byClass["Precision"]
nnet_recall <- nnet_sensitivity
nnet_f1 <- nnet_cm$byClass["F1"]

nnet_roc <- roc(response = nnet_obs, predictor = nnet_probs)
nnet_auc <- auc(nnet_roc)

eval_df <- rbind(eval_df, data.frame(
  Model       = "Neural Network",
  Accuracy    = as.numeric(nnet_accuracy),
  Error_Rate  = as.numeric(nnet_error_rate),
  Specificity = as.numeric(nnet_specificity),
  Sensitivity = as.numeric(nnet_sensitivity),
  Precision   = as.numeric(nnet_precision),
  Recall      = as.numeric(nnet_recall),
  F1_Score    = as.numeric(nnet_f1),
  AUC         = as.numeric(nnet_auc),
  stringsAsFactors = FALSE
))

#Print Confusion Matrix 
nnet_cm

#Plot ROC Curve
plot(nnet_roc, main = "Neural Network ROC Curve", col = "steelblue", lwd = 3, xlim = c(1,0), ylim = c(0,1))
abline(a = 0, b = 1, lty = 2, col = "gray")  
text(0.6, 0.2, paste0("AUC = ", round(nnet_auc, 3)), cex = 1.2)

```
### RANDOM FOREST

```{r}
# Train Random Forest Model using caret
set.seed(123)
rf_model <- train(
  x = X_train,
  y = y_train,
  method = "rf",
  trControl = ctrl,
  metric = "ROC"
)

# Predict on validation set
rf_preds <- predict(rf_model, X_val)
rf_probs <- predict(rf_model, X_val, type = "prob")[, "Yes"]

# Evaluate performance
rf_cm <- confusionMatrix(rf_preds, y_val, positive = "Yes")

rf_accuracy <- rf_cm$overall["Accuracy"]
rf_error_rate <- 1 - rf_accuracy
rf_sensitivity <- rf_cm$byClass["Sensitivity"]
rf_specificity <- rf_cm$byClass["Specificity"]
rf_precision <- rf_cm$byClass["Precision"]
rf_recall <- rf_sensitivity
rf_f1 <- rf_cm$byClass["F1"]

rf_roc <- roc(response = y_val, predictor = rf_probs)
rf_auc <- auc(rf_roc)

# Add to evaluation data frame
eval_df <- rbind(eval_df, data.frame(
  Model       = "Random Forest",
  Accuracy    = as.numeric(rf_accuracy),
  Error_Rate  = as.numeric(rf_error_rate),
  Specificity = as.numeric(rf_specificity),
  Sensitivity = as.numeric(rf_sensitivity),
  Precision   = as.numeric(rf_precision),
  Recall      = as.numeric(rf_recall),
  F1_Score    = as.numeric(rf_f1),
  AUC         = as.numeric(rf_auc),
  stringsAsFactors = FALSE
))

# Print Confusion Matrix
rf_cm

# Plot ROC Curve
plot(rf_roc, main = "Random Forest ROC Curve", col = "darkgreen", lwd = 3, xlim = c(1,0), ylim = c(0,1))
abline(a = 0, b = 1, lty = 2, col = "gray")
text(0.6, 0.2, paste0("AUC = ", round(rf_auc, 3)), cex = 1.2)

```
### \*\*\*\*\*\*\*\*\*\*\*\***Add Additional Models here**

## Model Evaluation 

```{r}
eval_df
```
