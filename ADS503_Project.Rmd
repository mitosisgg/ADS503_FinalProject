---
title: "ADS503_Project"
author: "Christian Lee, Askhat Patni, Gagandeep Singh"
date: "2025-06-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ADS-503 Final Project

### Import Libraries

```{r, message=FALSE, warning=FALSE}
library(tidyverse)    
library(skimr)        
library(corrplot)     
library(GGally)       
library(patchwork)  
library(gridExtra)
library(DataExplorer)
library(randomForest)
library(caret)
library(pROC)
```

## Load Dataset

```{r}
sleep_data <- read.csv("data/Sleep_health_and_lifestyle_dataset.csv")

head(sleep_data)
```

## Exploratory Data Analysis

```{r}
# Summarize Data Set
skim(sleep_data)
```

```{r}
#Numeric Feature Distributions
sleep_data |>
  select(where(is.numeric)) |>
  gather() |>
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  geom_histogram(fill = "steelblue", color = "black", bins = 20) +
  theme_minimal()
```

```{r}
#Categorical Feature Distributions
sleep_data |>
  select(where(is.factor), Gender, Occupation, BMI.Category, Sleep.Disorder) |>
  gather() |>
  ggplot(aes(x = value)) +
  facet_wrap(~ key, scales = "free", ncol = 2) +
  geom_bar(fill = "steelblue", color = "black",) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#Correlation Matrix 
numeric_data <- sleep_data |>
  select(where(is.numeric))

cor_matrix <- cor(numeric_data, use = "complete.obs")

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.cex = 0.8)
```

```{r}
#Create Binary outcome column 
sleep_data <- sleep_data |>
  mutate(Has.Sleep.Disorder = ifelse(Sleep.Disorder == "None", 0, 1))

# Inspect class ratio
table(sleep_data$Has.Sleep.Disorder)
```

```{r}
#Show relationship between numeric predictors and Binary outcome
numeric_features <- sleep_data |>
  select(where(is.numeric)) |>
  select(-Person.ID, -Has.Sleep.Disorder) |>
  names()

plot_list <- lapply(numeric_features, function(feature) {
  ggplot(sleep_data, aes(x = factor(Has.Sleep.Disorder), y = .data[[feature]], fill = factor(Has.Sleep.Disorder))) +
    geom_boxplot() +
    labs(title = feature, x = "Has Sleep Disorder", y = feature) +
    theme_minimal() +
    theme(legend.position = "none")
})

grid.arrange(grobs = plot_list, ncol = 3, nrow = ceiling(length(plot_list)/3))
```

## Feature Engineering
```{r}
# create systolic and diastolic features from Blood.Pressure
split_bp <- do.call(rbind, strsplit(sleep_data$Blood.Pressure, "/"))
sleep_data$Systolic <- as.numeric(split_bp[, 1])
sleep_data$Diastolic <- as.numeric(split_bp[, 2])

# create dummy variables for categorical features (Gender, Occupation, BMI.Category)
predictors <- sleep_data |> select(-Has.Sleep.Disorder)
dummy_model <- dummyVars(~ Gender + Occupation + BMI.Category, data = predictors)

# Apply it to your data
dummy_data <- predict(dummy_model, newdata = predictors)

# Convert to data frame if needed
dummy_data <- as.data.frame(dummy_data)

# Add encoded data back to the data set
df <- bind_cols(dummy_data, sleep_data)

# drop the unused features
df <- df |> select(-Blood.Pressure, -Person.ID, -Gender, -Occupation, -BMI.Category, -Sleep.Disorder)
```



## Data splitting

```{r}
# Data splitting
# Set seed for reproducibility
set.seed(123)

# Initial stratified split: 75% for training + validation, 25% for testing
train_idx <- createDataPartition(df$Has.Sleep.Disorder, p = 0.75, list = FALSE)
train_val_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

# Second stratified split: 80% of train_val_data for training, 20% for validation
val_idx <- createDataPartition(train_val_data$Has.Sleep.Disorder, p = 0.8, list = FALSE)
train_data <- train_val_data[val_idx, ]
val_data <- train_val_data[-val_idx, ]

# Check proportions
prop.table(table(train_data$Has.Sleep.Disorder))
prop.table(table(val_data$Has.Sleep.Disorder))
prop.table(table(test_data$Has.Sleep.Disorder))
```

## Model Development 

### Setup 

```{r}
# separate predictors from target in train, val and test sets
X_train <- train_data |> select(-Has.Sleep.Disorder) 
y_train <- train_data |> pull(Has.Sleep.Disorder) |> factor(levels = c(0, 1), labels = c("No", "Yes"))

X_val <- val_data |> select(-Has.Sleep.Disorder)
y_val <- val_data |> pull(Has.Sleep.Disorder) |> factor(levels = c(0, 1), labels = c("No", "Yes"))

X_test <- test_data |> select(-Has.Sleep.Disorder)
y_test <- test_data |> pull(Has.Sleep.Disorder) |> factor(levels = c(0, 1), labels = c("No", "Yes"))

# define train controls
ctrl <- trainControl(method = "cv",
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

# evaluation metrics data frame                     
eval_df <- data.frame(
  Model = character(),
  Accuracy = numeric(),
  Error_Rate = numeric(),
  Specificity = numeric(),
  Sensitivity = numeric(),
  Precision = numeric(),
  Recall = numeric(),
  F1_Score = numeric(),
  AUC = numeric(),
  stringsAsFactors = FALSE
)
```

### Logistic Regression

```{r, message=FALSE, warning=FALSE}
# Train the model
logit_model <- train(
     x = X_train,
     y = y_train,
     metric = "ROC",
     method = "glm",
     family = "binomial",
     preProcess = c("zv", "center", "scale"),
     trControl = ctrl
)

# Confusion Matrix
logit_cm <- confusionMatrix(
  logit_model$pred$pred, 
  logit_model$pred$obs
)

#Predict on val data
logit_preds <- predict(logit_model, X_val)
logit_obs <- y_val
logit_preds <- factor(logit_preds, levels = c("No", "Yes"))
logit_obs   <- factor(logit_obs,   levels = c("No", "Yes"))

logit_probs <- predict(logit_model, X_val, type = "prob")[, "Yes"]

logit_accuracy <- logit_cm$overall["Accuracy"]
logit_error_rate <- 1 - logit_accuracy
logit_sensitivity <- logit_cm$byClass["Sensitivity"]
logit_specificity <- logit_cm$byClass["Specificity"]
logit_precision <- logit_cm$byClass["Precision"]
logit_recall <- logit_sensitivity
logit_f1 <- logit_cm$byClass["F1"]

# ROC
logit_roc <- roc(logit_model$pred$obs, logit_model$pred$Yes)
logit_auc <- auc(logit_roc)

# Add eval metrics to summary df
eval_df <- rbind(eval_df, data.frame(
  Model       = "Logistic Regression",
  Accuracy    = as.numeric(logit_accuracy),
  Error_Rate  = as.numeric(logit_error_rate),
  Specificity = as.numeric(logit_specificity),
  Sensitivity = as.numeric(logit_sensitivity),
  Precision   = as.numeric(logit_precision),
  Recall      = as.numeric(logit_recall),
  F1_Score    = as.numeric(logit_f1),
  AUC         = as.numeric(logit_auc),
  stringsAsFactors = FALSE
))

# Print confusion matrix
logit_cm

#Plot ROC Curve
plot(logit_roc, main = "Logit ROC Curve", col = "purple", lwd = 3, xlim = c(1,0), ylim = c(0,1))
abline(a = 0, b = 1, lty = 2, col = "gray")  
text(0.6, 0.2, paste0("AUC = ", round(logit_auc, 3)), cex = 1.2)
```

### Neural Network

```{r, message=FALSE, warning=FALSE}
#Train Model
nnetGrid <- expand.grid(decay = c(0, 0.01, .1, .5, 1), 
                        size = c(1, 3, 7, 11, 13))
max_size <- max(nnetGrid$size)

num_inputs <- ncol(X_train)  
num_hidden <- max_size       
num_outputs <- length(levels(y_train))  

max_weights_needed <- (num_inputs + 1) * num_hidden + (num_hidden + 1) * num_outputs

set.seed(123)
nnet_model <- train(x = X_train, y = y_train,
                    method = "nnet",
                    preProcess = c("center", "scale"),
                    tuneGrid = nnetGrid,
                    trControl = ctrl,
                    metric = "ROC",
                    linout = FALSE,
                    trace = FALSE,
                    MaxNWts = max_weights_needed,
                    maxit = 1000)

#Predict on val data
nnet_preds <- predict(nnet_model, X_val)
nnet_obs <- y_val
nnet_preds <- factor(nnet_preds, levels = c("No", "Yes"))
nnet_obs   <- factor(nnet_obs,   levels = c("No", "Yes"))

nnet_probs <- predict(nnet_model, X_val, type = "prob")[, "Yes"]

#Calculate metrics for eval_df
nnet_cm <- confusionMatrix(nnet_preds, nnet_obs, positive = "Yes")

nnet_accuracy <- nnet_cm$overall["Accuracy"]
nnet_error_rate <- 1 - nnet_accuracy
nnet_sensitivity <- nnet_cm$byClass["Sensitivity"]
nnet_specificity <- nnet_cm$byClass["Specificity"]
nnet_precision <- nnet_cm$byClass["Precision"]
nnet_recall <- nnet_sensitivity
nnet_f1 <- nnet_cm$byClass["F1"]

nnet_roc <- roc(response = nnet_obs, predictor = nnet_probs)
nnet_auc <- auc(nnet_roc)

eval_df <- rbind(eval_df, data.frame(
  Model       = "Neural Network",
  Accuracy    = as.numeric(nnet_accuracy),
  Error_Rate  = as.numeric(nnet_error_rate),
  Specificity = as.numeric(nnet_specificity),
  Sensitivity = as.numeric(nnet_sensitivity),
  Precision   = as.numeric(nnet_precision),
  Recall      = as.numeric(nnet_recall),
  F1_Score    = as.numeric(nnet_f1),
  AUC         = as.numeric(nnet_auc),
  stringsAsFactors = FALSE
))

#Print Confusion Matrix 
nnet_cm

#Plot ROC Curve
plot(nnet_roc, main = "Neural Network ROC Curve", col = "steelblue", lwd = 3, xlim = c(1,0), ylim = c(0,1))
abline(a = 0, b = 1, lty = 2, col = "gray")  
text(0.6, 0.2, paste0("AUC = ", round(nnet_auc, 3)), cex = 1.2)
```

### RANDOM FOREST

```{r}
# Train Random Forest Model using caret
set.seed(123)
rf_model <- train(
  x = X_train,
  y = y_train,
  method = "rf",
  trControl = ctrl,
  metric = "ROC"
)

# Predict on validation set
rf_preds <- predict(rf_model, X_val)
rf_probs <- predict(rf_model, X_val, type = "prob")[, "Yes"]

# Evaluate performance
rf_cm <- confusionMatrix(rf_preds, y_val, positive = "Yes")

rf_accuracy <- rf_cm$overall["Accuracy"]
rf_error_rate <- 1 - rf_accuracy
rf_sensitivity <- rf_cm$byClass["Sensitivity"]
rf_specificity <- rf_cm$byClass["Specificity"]
rf_precision <- rf_cm$byClass["Precision"]
rf_recall <- rf_sensitivity
rf_f1 <- rf_cm$byClass["F1"]

rf_roc <- roc(response = y_val, predictor = rf_probs)
rf_auc <- auc(rf_roc)

# Add to evaluation data frame
eval_df <- rbind(eval_df, data.frame(
  Model       = "Random Forest",
  Accuracy    = as.numeric(rf_accuracy),
  Error_Rate  = as.numeric(rf_error_rate),
  Specificity = as.numeric(rf_specificity),
  Sensitivity = as.numeric(rf_sensitivity),
  Precision   = as.numeric(rf_precision),
  Recall      = as.numeric(rf_recall),
  F1_Score    = as.numeric(rf_f1),
  AUC         = as.numeric(rf_auc),
  stringsAsFactors = FALSE
))

# Print Confusion Matrix
rf_cm

# Plot ROC Curve
plot(rf_roc, main = "Random Forest ROC Curve (Train Data)", col = "darkgreen", lwd = 3, xlim = c(1,0), ylim = c(0,1))
abline(a = 0, b = 1, lty = 2, col = "gray")
text(0.6, 0.2, paste0("AUC = ", round(rf_auc, 3)), cex = 1.2)

```
```{r}
# Predict on test set
rf_test_preds <- predict(rf_model, X_test)
rf_test_probs <- predict(rf_model, X_test, type = "prob")[, "Yes"]

# Evaluate performance on test set
rf_test_cm <- confusionMatrix(rf_test_preds, y_test, positive = "Yes")

rf_test_accuracy <- rf_test_cm$overall["Accuracy"]
rf_test_error_rate <- 1 - rf_test_accuracy
rf_test_sensitivity <- rf_test_cm$byClass["Sensitivity"]
rf_test_specificity <- rf_test_cm$byClass["Specificity"]
rf_test_precision <- rf_test_cm$byClass["Precision"]
rf_test_recall <- rf_test_sensitivity
rf_test_f1 <- rf_test_cm$byClass["F1"]

rf_test_roc <- roc(response = y_test, predictor = rf_test_probs)
rf_test_auc <- auc(rf_test_roc)

# Print Confusion Matrix
rf_test_cm

# Plot ROC Curve for test set
plot(rf_test_roc, main = "Random Forest ROC Curve (Test Data)", col = "darkred", lwd = 3, xlim = c(1, 0), ylim = c(0, 1))
abline(a = 0, b = 1, lty = 2, col = "gray")
text(0.6, 0.2, paste0("AUC = ", round(rf_test_auc, 3)), cex = 1.2)
```

## Model Evaluation 
```{r}
eval_df |> arrange(desc(F1_Score))

eval_long <- eval_df |>
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")

# Plot
ggplot(eval_long, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Model Performance Comparison",
       x = "Metric",
       y = "Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
